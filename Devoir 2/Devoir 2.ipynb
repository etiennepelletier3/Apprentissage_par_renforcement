{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environnements simples et solutions exactes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Implémentation d'un environnement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérez l'environnement en grille $5 \\times 5$ suivant :\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\text{A} & \\phantom{\\text{A}} & \\text{B} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\text{B'} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\text{A'} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "L'environnement est caractérisé par l'ensemble d'actions $\\mathcal A := \\{ \\uparrow, \\downarrow, \\rightarrow, \\leftarrow \\}$ visant à déplacer l'agent d'une case dans la direction indiquée par la flèche.\n",
    "\n",
    "Considérez des déplacements déterministes :\n",
    "- si $S_t = \\text{A}$, alors $S_{t+1} = \\text{A'}$ (peu importe l'action);\n",
    "- si $S_t = \\text{B}$, alors $S_{t+1} = \\text{B'}$ (peu importe l'action);\n",
    "- si l'action fait sortir l'agent de la grille, alors $S_{t+1} = S_t$;\n",
    "- sinon appliquer l'action.\n",
    "\n",
    "Considérez la fonction de récompense suivante :\n",
    "- si $S_t = \\text{A}$, alors $R_{t+1} = R_{\\text{A}}$;\n",
    "- si $S_t = \\text{B}$, alors $R_{t+1} = R_{\\text{B}}$;\n",
    "- si l'action fait sortir l'agent de la grille, alors $R_{t+1} = -1$;\n",
    "- sinon $R_{t+1} = 0$,\n",
    "\n",
    "où $R_{\\text{A}}$ et $R_{\\text{B}}$ sont des valeurs spécifiées à l'initialisation de l'environnement.\n",
    "\n",
    "Complétez le code suivant pour implémenter cet environnement. Votre classe doit contenir une fonction _expected\\_reward_ retournant la récompense attendue en suivant la politique spécifiée dans un état donné. Votre classe doit également contenir une fonction _prob\\_transition\\_policy_ retournant la probabilité de transitionner vers un prochain état donné en suivant la politique spécifiée dans un état courant donné.\n",
    "\n",
    "Vous pouvez implémenter d'autres fonctions utilitaires à votre guise. Les manières de représenter les états et la politique sont laissées à votre discrétion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    \n",
    "    def __init__(self, reward_a, reward_b):\n",
    "        self.reward_a = reward_a\n",
    "        self.reward_b = reward_b\n",
    "        \n",
    "        self.rows = 5\n",
    "        self.cols = 5\n",
    "\n",
    "        self.A = (0, 1)      # position de A\n",
    "        self.A_prime = (4, 1)  # position de A'\n",
    "        self.B = (0, 3)      # position de B\n",
    "        self.B_prime = (2, 3)  # position de B'\n",
    "\n",
    "        self.actions = {\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0),\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1)\n",
    "        }\n",
    "\n",
    "    def step(self, state, action):\n",
    "\n",
    "        # Si state est A ou B\n",
    "        if state == self.A:\n",
    "            return self.A_prime, self.reward_a\n",
    "        if state == self.B:\n",
    "            return self.B_prime, self.reward_b\n",
    "        \n",
    "        # Mouvement standard : calcul de la nouvelle position\n",
    "        delta = self.actions[action]\n",
    "        next_state = (state[0] + delta[0], state[1] + delta[1])\n",
    "        \n",
    "        # Vérifier si le nouvel état est hors grille\n",
    "        if not (0 <= next_state[0] < self.rows and 0 <= next_state[1] < self.cols):\n",
    "            # Si l'agent sort de la grille, il reste au même endroit et reçoit -1\n",
    "            return state, -1\n",
    "        \n",
    "        # Sinon, le déplacement est normal et la récompense est nulle\n",
    "        return next_state, 0\n",
    "    \n",
    "    def expected_reward(self, state, policy):\n",
    "        # Pour les états A et B, la récompense est déterminée indépendamment de l'action\n",
    "        if state == self.A:\n",
    "            return self.reward_a\n",
    "        if state == self.B:\n",
    "            return self.reward_b\n",
    "    \n",
    "        exp_reward = 0\n",
    "        # Parcourir toutes les actions possibles dans l'état selon la politique\n",
    "        for action, prob in policy[state].items():\n",
    "            _, reward = self.step(state, action)\n",
    "            exp_reward += prob * reward\n",
    "        return exp_reward\n",
    "    \n",
    "    def prob_transition_policy(self, next_state, state, policy):\n",
    "        # Pour les états spéciaux A et B, le prochain état est fixé\n",
    "        if state == self.A:\n",
    "            return 1 if next_state == self.A_prime else 0\n",
    "        if state == self.B:\n",
    "            return 1 if next_state == self.B_prime else 0\n",
    "        \n",
    "        total_prob = 0\n",
    "        # Somme des probabilités de toutes les actions conduisant à next_state\n",
    "        for action, prob in policy[state].items():\n",
    "            ns, _ = self.step(state, action)\n",
    "            if ns == next_state:\n",
    "                total_prob += prob\n",
    "        return total_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expliquez comment vous représentez une politique et comment vous représentez les états dans votre implémentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les états sont représentés par un tuple (i, j) où i représente l'indice de la ligne et j celui de la colonne. Ils sont tous une position dans la grille 5 x 5. L'état A est représenté par le tuple (0, 1) et A' par (4, 1).\n",
    "\n",
    "La politique est représentée par un dictionnaire où les clés sont les états possibles dans la grille et les valeurs sont un dictionnaire des 4 mouvements possibles avec leur probabilité associée dans cet état."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10] points] Résolution exacte du système linéaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résolvez le système linéaire résultant de cet environnement pour calculer la fonction de valeur d'état associée à la politique $\\pi(a | s) = 1/4$ pour tout $a \\in \\mathcal A$, $s \\in \\mathcal S$. Considérez un taux d'actualisation $\\gamma = 0.9$.\n",
    "\n",
    "Affichez votre résultat sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.309  8.789  4.428  5.322  1.492]\n",
      " [ 1.522  2.992  2.25   1.908  0.547]\n",
      " [ 0.051  0.738  0.673  0.358 -0.403]\n",
      " [-0.974 -0.435 -0.355 -0.586 -1.183]\n",
      " [-1.858 -1.345 -1.229 -1.423 -1.975]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "reward_A = 10\n",
    "reward_B = 5\n",
    "\n",
    "grid = Gridworld(reward_A, reward_B)\n",
    "\n",
    "states = [(i, j) for i in range(grid.rows) for j in range(grid.cols)]\n",
    "n_states = len(states)\n",
    "\n",
    "def state_to_index(state):\n",
    "    \"\"\"Associe à un état (i,j) son indice dans [0, n_states - 1].\"\"\"\n",
    "    i, j = state\n",
    "    return i * grid.cols + j\n",
    "\n",
    "# --- Construction du système linéaire A * V = b ---\n",
    "A_mat = numpy.zeros((n_states, n_states))\n",
    "b_vec = numpy.zeros(n_states)\n",
    "\n",
    "for state in states:\n",
    "    idx = state_to_index(state)\n",
    "    A_mat[idx, idx] = 1\n",
    "    # Cas particuliers des états A et B (transitions déterministes)\n",
    "    if state == grid.A:\n",
    "        A_mat[idx, state_to_index(grid.A_prime)] = -gamma\n",
    "        b_vec[idx] = reward_A\n",
    "    elif state == grid.B:\n",
    "        A_mat[idx, state_to_index(grid.B_prime)] = -gamma\n",
    "        b_vec[idx] = reward_B\n",
    "\n",
    "    else:\n",
    "        # Pour un état ordinaire, on part de l'équation : \n",
    "        for action in grid.actions.keys():\n",
    "            next_state, r = grid.step(state, action)\n",
    "            idx_ns = state_to_index(next_state)\n",
    "            A_mat[idx, idx_ns] -= gamma/4\n",
    "            b_vec[idx] += r/4\n",
    "\n",
    "V = numpy.linalg.solve(A_mat, b_vec)\n",
    "V_matrix = V.reshape(grid.rows, grid.cols)\n",
    "print(numpy.round(V_matrix, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmation dynamique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Évaluation de politique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le code suivant pour implémenter la stratégie d'évaluation de politique par programmation dynamique. Votre fonction doit recevoir en entrée l'environnement à résoudre, la politique à évaluer, le taux d'actualisation $\\gamma$, ainsi qu'un seuil de tolérance à l'erreur $\\theta$ en-dessous duquel l'algorithme peut terminer. Votre fonction doit retourner la fonction de valeur d'état sous la forme d'un vecteur de longueur $|\\mathcal S|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(gridworld, pi, gamma, theta):\n",
    "    rows, cols = gridworld.rows, gridworld.cols\n",
    "    n_states = rows * cols\n",
    "    V = numpy.zeros(n_states)\n",
    "\n",
    "    delta = float('inf')\n",
    "    while delta >= theta:\n",
    "        delta = 0\n",
    "        # Parcourir tous les états\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                state = (i, j)\n",
    "                idx = state_to_index(state)\n",
    "                v = V[idx]\n",
    "                new_v = 0\n",
    "                # Calcul de l'espérance sur toutes les actions selon la politique pi\n",
    "                for action, prob in pi[state].items():\n",
    "                    next_state, reward = gridworld.step(state, action)\n",
    "                    new_v += prob * (reward + gamma * V[state_to_index(next_state)])\n",
    "                V[idx] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "    return V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez votre méthode pour évaluer la valeur des états sous la politique $\\pi(a|s) = 1/4$ pour tout $a \\in \\mathcal A$, $s \\in \\mathcal S$, dans l'environnement en grille implémenté précédemment. Considérez un taux d'actualisation $\\gamma = 0.9$ et un seuil de tolérance $\\theta = 10^{-5}$.\n",
    "\n",
    "Affichez votre résultat sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.309  8.789  4.428  5.322  1.492]\n",
      " [ 1.522  2.992  2.25   1.908  0.547]\n",
      " [ 0.051  0.738  0.673  0.358 -0.403]\n",
      " [-0.974 -0.435 -0.355 -0.586 -1.183]\n",
      " [-1.858 -1.345 -1.229 -1.423 -1.975]]\n"
     ]
    }
   ],
   "source": [
    "reward_A = 10\n",
    "reward_B = 5\n",
    "gamma = 0.9\n",
    "theta = 1e-5\n",
    "\n",
    "grid = Gridworld(reward_A, reward_B)\n",
    "\n",
    "policy = {}\n",
    "for i in range(grid.rows):\n",
    "    for j in range(grid.cols):\n",
    "        # Pour chaque état (i,j), la probabilité de prendre n'importe quelle action est 1/4\n",
    "        policy[(i, j)] = {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
    "\n",
    "V = policy_evaluation(grid, policy, gamma, theta)\n",
    "\n",
    "# Conversion du vecteur V en une matrice 5x5 (ordre row-major)\n",
    "V_matrix = V.reshape((grid.rows, grid.cols))\n",
    "print(numpy.round(V_matrix, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Itération de valeur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le code suivant pour implémenter la stratégie d'itération de valeurs permettant d'identifier la fonction de valeur de la politique optimale par programmation dynamique. Votre fonction doit recevoir en entrée l'environnement à résoudre, le taux d'actualisation $\\gamma$, ainsi qu'un seuil de tolérance à l'erreur $\\theta$ en-dessous duquel l'algorithme peut terminer. Votre fonction doit retourner la fonction de valeur d'état sous la forme d'un vecteur de longueur $|\\mathcal S|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gridworld, gamma, theta):\n",
    "    rows, cols = gridworld.rows, gridworld.cols\n",
    "    n_states = rows * cols\n",
    "    V = numpy.zeros(n_states)\n",
    "\n",
    "    delta = float('inf')\n",
    "    while delta >= theta:\n",
    "        delta = 0\n",
    "        # Parcourir tous les états\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                state = (i, j)\n",
    "                idx = state_to_index(state)\n",
    "                v = V[idx]\n",
    "                \n",
    "                # Estimation de la valeur optimale pour l'état courant\n",
    "                v_actions = []\n",
    "                for action in gridworld.actions.keys(): # Parcourir toutes les actions possibles\n",
    "                    next_state, reward = gridworld.step(state, action)\n",
    "                    v_actions.append(reward + gamma * V[state_to_index(next_state)]) # Estimation de la valeur pour l'action courante\n",
    "\n",
    "                v_optim = max(v_actions) # Valeur optimale pour l'état courant\n",
    "                V[idx] = v_optim # Mise à jour de la valeur de l'état courant\n",
    "                delta = max(delta, abs(v - v_optim)) # Mise à jour de delta\n",
    "    return V               \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez votre méthode pour évaluer la valeur des états sous la politique optimale dans l'environnement en grille implémenté précédemment. Considérez un taux d'actualisation $\\gamma = 0.9$ et un seuil de tolérance $\\theta = 10^{-5}$.\n",
    "\n",
    "Affichez votre résultat sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.977 24.419 21.977 19.419 17.477]\n",
      " [19.78  21.977 19.78  17.802 16.022]\n",
      " [17.802 19.78  17.802 16.022 14.419]\n",
      " [16.022 17.802 16.022 14.419 12.977]\n",
      " [14.419 16.022 14.419 12.977 11.68 ]]\n"
     ]
    }
   ],
   "source": [
    "reward_A = 10\n",
    "reward_B = 5\n",
    "gamma = 0.9\n",
    "theta = 1e-5\n",
    "\n",
    "grid = Gridworld(reward_A, reward_B)\n",
    "\n",
    "V = value_iteration(grid, gamma, theta)\n",
    "\n",
    "# Conversion du vecteur V en une matrice 5x5 (ordre row-major)\n",
    "V_matrix = V.reshape((grid.rows, grid.cols))\n",
    "print(numpy.round(V_matrix, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Énumérez toutes les situations dans lesquelles un agent suivant la politique optimale traverse l'état $\\text{B}$. Justifiez votre réponse en référant aux valeurs d'états affichées précédemment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux seules situations qui mènent à traverser l’état B sont les suivantes : soit la position (0, 3), c’est-à-dire B directement, soit la position (0, 4).\n",
    "\n",
    "Dans toutes les autres positions, la politique optimale estime qu’il est plus avantageux de se diriger vers l’état A, qui rapporte une récompense plus élevée. Ainsi, la valeur des états en se rapprochant de A est toujours supérieure à celle des états menant vers B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Évaluez maintenant la valeur des états sous la politique optimale en considérant un taux d'actualisation $\\gamma = 0.2$ (conserver le même seuil de tolérance $\\theta = 10^{-5}$).\n",
    "\n",
    "Affichez votre résultat sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Expliquez l'impact de cette réduction taux d'actualisation sur la politique optimale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Quelle est la valeur entière minimale de $R_{\\text{A}}$ permettant de retrouver, avec $\\gamma = 0.2$, la même politique optimale que celle calculée précédemment avec $\\gamma = 0.9$?\n",
    "\n",
    "Supportez votre réponse en affichant les valeurs d'états sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ L'expérience précédente permet d'exposer une relation entre l'amplitude des récompenses, la valeur du taux d'actualisation et la politique optimale résultante. Décrivez cette relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrôle tabulaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Environnement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant permet d'instancier un environnement de type CartPole-v1 provenant de la librairie Gymnasium d'OpenAI :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "environment = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet environnement, le but de l'agent consiste à déplacer une plateforme de manière à faire tenir en équilibre un poteau sur la plateforme. Les états sont des vecteurs représentant la position de la plateforme et du poteau. La récompense est de 1 pour chaque état où le poteau est encore sur la plateforme et tombe à 0 lorsque le poteau tombe, ce qui sonne la fin de l'épisode.\n",
    "\n",
    "L'environnement dispose de différents attributs permettant, par exemple, de retrouver la dimension d'un état $s \\in \\mathcal S$ et la dimension de l'espace d'actions $\\mathcal A$ dans l'environnement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'environnement CartPole-v1 a des états de la forme (4,) et contient 2 actions.\n"
     ]
    }
   ],
   "source": [
    "# observation_space : environment attribute qualifying the state space\n",
    "# continuous state space : shape attribute\n",
    "observation_dim = environment.observation_space.shape\n",
    "\n",
    "# action_space : environment attribute qualifying the action space\n",
    "# discrete action space : n attribute indicating the number of actions\n",
    "# actions : 0...(n-1)\n",
    "n_actions = environment.action_space.n # only discrete action spaces\n",
    "\n",
    "print(f\"L'environnement {environment.spec.id} a des états de la forme {observation_dim} et contient {n_actions} actions.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À quoi correspondent les éléments d'un vecteur d'état dans cet environnement?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selon la documentation de https://gymnasium.farama.org/environments/classic_control/cart_pole, les éléments d'un vecteur d'état sont (par ordre croissant d'index) :\n",
    "\n",
    "0. Position de la platforme $x \\in [-4.8, 4.8]$\n",
    "1. Vitesse de la platforme $v \\in \\ ]-\\infty, \\infty[$\n",
    "2. Angle du poteau $\\theta \\in [-0.418, 0.418]$\n",
    "3. Vitesse angulaire du poteau $\\omega \\in \\ ]-\\infty, \\infty[$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les besoins de l'implémentation tabulaire, il est nécessaire de discrétiser l'espace d'états continu de CartPole. Le code suivant permet de déterminer les limites de l'espace d'état :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians\n",
    "\n",
    "# get upper bounds on state components\n",
    "upper_bounds = environment.observation_space.high\n",
    "\n",
    "# recall : [cart position, cart speed, pole angle, pole speed]\n",
    "upper_bounds[1] = 0.5\n",
    "upper_bounds[3] = radians(50)\n",
    "\n",
    "# symmetric state space\n",
    "lower_bounds = -upper_bounds # or environment.observation_space.low with similar trick as above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme les vitesses ont des bornes infinies qui ne se prètent pas bien à la discrétisation, on leur fixe les valeurs limites de 0.5 unité / $t$ et 50 radians / $t$ pour la vitesse du chariot et du poteau, respectivement.\n",
    "\n",
    "Complétez le code suivant pour implémenter une fonction permettant de discrétiser un état. Votre fonction doit recevoir en entrée l'état à discrétiser, les bornes inférieures et supérieures à considérer pour chaque composante d'un état, ainsi  qu'une liste indiquant le nombre de zones (_buckets_ ou _bins_) à utiliser pour discrétiser chaque composante. Chaque zone est de taille identique : $(u_i - \\ell_i) / N_i$, où $u_i$ et $\\ell_i$ sont les bornes supérieure et inférieure pour la composante $i$ et $N_i$ est le nombre de zones qui divisent la composante $i$. Votre fonction doit retourner un tuple de taille 4, où chaque indice correspond à l'indice de la zone discrète représentant la composante continue Assurez-vous de gérer les cas limites où l'état en entrée contient des composantes en dehors de vos bornes (ramenez-les à la borne la plus près). Considérez la discrétisation par défaut en 1, 1, 6 et 12 zones par composante respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(state, lower_bounds, upper_bounds, n_buckets=[1, 1, 6, 12]):\n",
    "    '''\n",
    "    state : (4,) numpy array\n",
    "    lower_bounds : (4,) numpy array\n",
    "    upper_bounds : (4,) numpy array\n",
    "    n_buckets : list of integers\n",
    "\n",
    "    Return : Discrete state index (integer).\n",
    "    '''\n",
    "    discrete_state = ()\n",
    "\n",
    "    for c_i in range(len(state)):\n",
    "        if state[c_i] <= lower_bounds[c_i]:\n",
    "            index = 0\n",
    "        elif state[c_i] >= upper_bounds[c_i]:\n",
    "            index = n_buckets[c_i] - 1\n",
    "        else:\n",
    "            # Créer des zones espacées uniformément\n",
    "            zones = numpy.linspace(lower_bounds[c_i], upper_bounds[c_i], n_buckets[c_i] + 1)\n",
    "            # Trouver la zone dans laquelle se trouve l'état\n",
    "            index = numpy.digitize(state[c_i], zones) - 1  # Soustrait 1 pour démarrer à 0\n",
    "\n",
    "        discrete_state += (int(index),)\n",
    "    \n",
    "    # Convertir le tuple en un index unique\n",
    "    state_index = 0\n",
    "    for i, val in enumerate(discrete_state):\n",
    "        state_index = state_index * n_buckets[i] + val\n",
    "\n",
    "    return state_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ La discrétisation suggérée est seulement appliquée sur la position et la vitesse du poteau. Pourquoi est-ce que cette discrétisation est suffisante pour l'apprentissage d'une bonne politique?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet de visualiser une politique _greedy_ définie sur des valeurs d'actions ($q$-_values_) données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tabular(q_values, max_steps=1000):\n",
    "    '''\n",
    "    q_values : (n, k) numpy array where n is the number of (discrete) states and k is the number of actions\n",
    "    '''\n",
    "    environment = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "    time_step = 0\n",
    "    terminated, truncated = False, False\n",
    "    state, _ = environment.reset()\n",
    "    s_idx = discretize(state, lower_bounds, upper_bounds)\n",
    "    while not (terminated or truncated) and time_step < max_steps:\n",
    "        environment.render()\n",
    "        action = numpy.argmax(q_values[s_idx])\n",
    "        next_state, _, terminated, truncated, _ = environment.step(action)            \n",
    "        next_s_idx = discretize(next_state, lower_bounds, upper_bounds)\n",
    "        s_idx = next_s_idx\n",
    "        time_step += 1\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elle sera utilisée dans les sections suivantes pour visualiser les politiques apprises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Apprentissage Monte Carlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le code suivant pour implémenter la stratégie First-visit Monte Carlo pour le contrôle. Votre fonction doit recevoir en entrée l'environnement à aborder, le taux d'actualisation $\\gamma$ à considérer dans l'objectif, ainsi que le nombre maximal d'épisodes à effectuer. Votre fonction doit retourner la matrice des valeurs d'actions ($q$-_values_) sous la forme d'un _numpy.array_ de dimension $|\\mathcal S| \\times |\\mathcal A|$, où $\\mathcal S$ correspond à l'ensemble des états discrétisés.\n",
    "\n",
    "Considérez une politique de collecte de données de type $\\varepsilon$-greedy. Pour chaque épisode $i = 1...N$, considérez un taux d'exploration $\\varepsilon = 0.99^{i-1}$ tant que $\\varepsilon \\geq 0.1$.\n",
    "\n",
    "À tous les 10 épisodes, votre fonction doit afficher le taux d'exploration $\\varepsilon$ utilisé ainsi que la somme des récompenses cumulées (sans actualisation) durant l'épisode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_control(environment, gamma, max_episodes):\n",
    "    n_buckets = [1, 1, 6, 12]\n",
    "    n = numpy.prod(n_buckets)  # nombre total d'états discrets possibles\n",
    "    k = environment.action_space.n # nombre total d'actions possibles\n",
    "\n",
    "    q_values = numpy.random.rand(n, k)\n",
    "    alpha = 0.1 # taux d'apprentissage\n",
    "\n",
    "    for e in range(max_episodes):\n",
    "        epsilon = 0.99 ** e if 0.99 ** e > 0.1 else 0.1 # taux d'exploration\n",
    "        trajectoire = [] # séquence d'états-actions-récompenses durant l'épisode\n",
    "        total_reward = 0\n",
    "        fin_episode = False\n",
    "        state, _ = environment.reset()\n",
    "        s_idx = discretize(state, lower_bounds, upper_bounds)\n",
    "        while not fin_episode:\n",
    "            if numpy.random.rand() > epsilon:\n",
    "                action = numpy.argmax(q_values[s_idx]) \n",
    "            else:\n",
    "                action = numpy.random.randint(k)\n",
    "    \n",
    "            next_state, reward, terminated, truncated, _ = environment.step(action)\n",
    "            trajectoire.append((s_idx, action, reward))\n",
    "            total_reward += reward\n",
    "\n",
    "            next_s_idx = discretize(next_state, lower_bounds, upper_bounds)\n",
    "            s_idx = next_s_idx\n",
    "            fin_episode = terminated or truncated\n",
    "        \n",
    "        # mise à jour des q_values\n",
    "        s_a_vu = set() # états-actions rencontrés une fois\n",
    "        for t in range(len(trajectoire)):\n",
    "            s, a, _ = trajectoire[t]\n",
    "            if (s, a) not in s_a_vu:\n",
    "                s_a_vu.add((s, a))\n",
    "                G_t = sum([gamma ** k * trajectoire[t + 1 + k][2] for k in range(len(trajectoire) - t - 1)]) # calcul du rendement actualisé\n",
    "  \n",
    "                # Mise à jour des q_values par taux d'apprentissage alpha (pas mentionné pourtant mais fonctionne super bien)\n",
    "                q_values[s][a] = q_values[s][a] + alpha * (G_t - q_values[s][a])\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            print(f\"Episode {e} : epsilon = {epsilon}, total_reward = {total_reward}\")\n",
    "    environment.close()\n",
    "\n",
    "    return q_values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquez votre stratégie First-visit Monte Carlo au contrôle de l'agent dans CartPole-v1.\n",
    "\n",
    "Générez 5000 épisodes sans utiliser d'actualisation (donc avec $\\gamma = 1$). Votre agent devrait converger à une récompense cumulative autour de 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 : epsilon = 1.0, total_reward = 14.0\n",
      "Episode 10 : epsilon = 0.9043820750088044, total_reward = 38.0\n",
      "Episode 20 : epsilon = 0.8179069375972308, total_reward = 122.0\n",
      "Episode 30 : epsilon = 0.7397003733882802, total_reward = 21.0\n",
      "Episode 40 : epsilon = 0.6689717585696803, total_reward = 17.0\n",
      "Episode 50 : epsilon = 0.6050060671375364, total_reward = 21.0\n",
      "Episode 60 : epsilon = 0.5471566423907612, total_reward = 97.0\n",
      "Episode 70 : epsilon = 0.49483865960020695, total_reward = 13.0\n",
      "Episode 80 : epsilon = 0.4475232137638106, total_reward = 24.0\n",
      "Episode 90 : epsilon = 0.4047319726783238, total_reward = 27.0\n",
      "Episode 100 : epsilon = 0.3660323412732292, total_reward = 255.0\n",
      "Episode 110 : epsilon = 0.33103308832101386, total_reward = 212.0\n",
      "Episode 120 : epsilon = 0.2993803913123313, total_reward = 202.0\n",
      "Episode 130 : epsilon = 0.27075425951199406, total_reward = 121.0\n",
      "Episode 140 : epsilon = 0.24486529903492948, total_reward = 500.0\n",
      "Episode 150 : epsilon = 0.22145178723886091, total_reward = 500.0\n",
      "Episode 160 : epsilon = 0.2002770268574893, total_reward = 293.0\n",
      "Episode 170 : epsilon = 0.18112695312597024, total_reward = 500.0\n",
      "Episode 180 : epsilon = 0.16380796970808742, total_reward = 133.0\n",
      "Episode 190 : epsilon = 0.14814499154757946, total_reward = 270.0\n",
      "Episode 200 : epsilon = 0.13397967485796172, total_reward = 500.0\n",
      "Episode 210 : epsilon = 0.12116881635704835, total_reward = 303.0\n",
      "Episode 220 : epsilon = 0.10958290556334815, total_reward = 232.0\n",
      "Episode 230 : epsilon = 0.1, total_reward = 310.0\n",
      "Episode 240 : epsilon = 0.1, total_reward = 338.0\n",
      "Episode 250 : epsilon = 0.1, total_reward = 290.0\n",
      "Episode 260 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 270 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 280 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 290 : epsilon = 0.1, total_reward = 235.0\n",
      "Episode 300 : epsilon = 0.1, total_reward = 213.0\n",
      "Episode 310 : epsilon = 0.1, total_reward = 245.0\n",
      "Episode 320 : epsilon = 0.1, total_reward = 310.0\n",
      "Episode 330 : epsilon = 0.1, total_reward = 403.0\n",
      "Episode 340 : epsilon = 0.1, total_reward = 72.0\n",
      "Episode 350 : epsilon = 0.1, total_reward = 282.0\n",
      "Episode 360 : epsilon = 0.1, total_reward = 345.0\n",
      "Episode 370 : epsilon = 0.1, total_reward = 298.0\n",
      "Episode 380 : epsilon = 0.1, total_reward = 237.0\n",
      "Episode 390 : epsilon = 0.1, total_reward = 191.0\n",
      "Episode 400 : epsilon = 0.1, total_reward = 196.0\n",
      "Episode 410 : epsilon = 0.1, total_reward = 8.0\n",
      "Episode 420 : epsilon = 0.1, total_reward = 47.0\n",
      "Episode 430 : epsilon = 0.1, total_reward = 53.0\n",
      "Episode 440 : epsilon = 0.1, total_reward = 75.0\n",
      "Episode 450 : epsilon = 0.1, total_reward = 35.0\n",
      "Episode 460 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 470 : epsilon = 0.1, total_reward = 376.0\n",
      "Episode 480 : epsilon = 0.1, total_reward = 179.0\n",
      "Episode 490 : epsilon = 0.1, total_reward = 348.0\n",
      "Episode 500 : epsilon = 0.1, total_reward = 452.0\n",
      "Episode 510 : epsilon = 0.1, total_reward = 429.0\n",
      "Episode 520 : epsilon = 0.1, total_reward = 286.0\n",
      "Episode 530 : epsilon = 0.1, total_reward = 301.0\n",
      "Episode 540 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 550 : epsilon = 0.1, total_reward = 367.0\n",
      "Episode 560 : epsilon = 0.1, total_reward = 316.0\n",
      "Episode 570 : epsilon = 0.1, total_reward = 249.0\n",
      "Episode 580 : epsilon = 0.1, total_reward = 340.0\n",
      "Episode 590 : epsilon = 0.1, total_reward = 331.0\n",
      "Episode 600 : epsilon = 0.1, total_reward = 251.0\n",
      "Episode 610 : epsilon = 0.1, total_reward = 291.0\n",
      "Episode 620 : epsilon = 0.1, total_reward = 221.0\n",
      "Episode 630 : epsilon = 0.1, total_reward = 334.0\n",
      "Episode 640 : epsilon = 0.1, total_reward = 315.0\n",
      "Episode 650 : epsilon = 0.1, total_reward = 191.0\n",
      "Episode 660 : epsilon = 0.1, total_reward = 296.0\n",
      "Episode 670 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 680 : epsilon = 0.1, total_reward = 454.0\n",
      "Episode 690 : epsilon = 0.1, total_reward = 98.0\n",
      "Episode 700 : epsilon = 0.1, total_reward = 324.0\n",
      "Episode 710 : epsilon = 0.1, total_reward = 474.0\n",
      "Episode 720 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 730 : epsilon = 0.1, total_reward = 492.0\n",
      "Episode 740 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 750 : epsilon = 0.1, total_reward = 400.0\n",
      "Episode 760 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 770 : epsilon = 0.1, total_reward = 387.0\n",
      "Episode 780 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 790 : epsilon = 0.1, total_reward = 467.0\n",
      "Episode 800 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 810 : epsilon = 0.1, total_reward = 412.0\n",
      "Episode 820 : epsilon = 0.1, total_reward = 220.0\n",
      "Episode 830 : epsilon = 0.1, total_reward = 350.0\n",
      "Episode 840 : epsilon = 0.1, total_reward = 317.0\n",
      "Episode 850 : epsilon = 0.1, total_reward = 296.0\n",
      "Episode 860 : epsilon = 0.1, total_reward = 353.0\n",
      "Episode 870 : epsilon = 0.1, total_reward = 399.0\n",
      "Episode 880 : epsilon = 0.1, total_reward = 475.0\n",
      "Episode 890 : epsilon = 0.1, total_reward = 322.0\n",
      "Episode 900 : epsilon = 0.1, total_reward = 288.0\n",
      "Episode 910 : epsilon = 0.1, total_reward = 428.0\n",
      "Episode 920 : epsilon = 0.1, total_reward = 475.0\n",
      "Episode 930 : epsilon = 0.1, total_reward = 214.0\n",
      "Episode 940 : epsilon = 0.1, total_reward = 402.0\n",
      "Episode 950 : epsilon = 0.1, total_reward = 354.0\n",
      "Episode 960 : epsilon = 0.1, total_reward = 479.0\n",
      "Episode 970 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 980 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 990 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1000 : epsilon = 0.1, total_reward = 327.0\n",
      "Episode 1010 : epsilon = 0.1, total_reward = 278.0\n",
      "Episode 1020 : epsilon = 0.1, total_reward = 391.0\n",
      "Episode 1030 : epsilon = 0.1, total_reward = 388.0\n",
      "Episode 1040 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1050 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1060 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1070 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1080 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1090 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1100 : epsilon = 0.1, total_reward = 444.0\n",
      "Episode 1110 : epsilon = 0.1, total_reward = 483.0\n",
      "Episode 1120 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1130 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1140 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1150 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1160 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1170 : epsilon = 0.1, total_reward = 405.0\n",
      "Episode 1180 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1190 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1200 : epsilon = 0.1, total_reward = 275.0\n",
      "Episode 1210 : epsilon = 0.1, total_reward = 288.0\n",
      "Episode 1220 : epsilon = 0.1, total_reward = 240.0\n",
      "Episode 1230 : epsilon = 0.1, total_reward = 461.0\n",
      "Episode 1240 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1250 : epsilon = 0.1, total_reward = 288.0\n",
      "Episode 1260 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1270 : epsilon = 0.1, total_reward = 310.0\n",
      "Episode 1280 : epsilon = 0.1, total_reward = 76.0\n",
      "Episode 1290 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1300 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1310 : epsilon = 0.1, total_reward = 173.0\n",
      "Episode 1320 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1330 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1340 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1350 : epsilon = 0.1, total_reward = 393.0\n",
      "Episode 1360 : epsilon = 0.1, total_reward = 480.0\n",
      "Episode 1370 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1380 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1390 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1400 : epsilon = 0.1, total_reward = 302.0\n",
      "Episode 1410 : epsilon = 0.1, total_reward = 342.0\n",
      "Episode 1420 : epsilon = 0.1, total_reward = 416.0\n",
      "Episode 1430 : epsilon = 0.1, total_reward = 428.0\n",
      "Episode 1440 : epsilon = 0.1, total_reward = 368.0\n",
      "Episode 1450 : epsilon = 0.1, total_reward = 267.0\n",
      "Episode 1460 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1470 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1480 : epsilon = 0.1, total_reward = 351.0\n",
      "Episode 1490 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1500 : epsilon = 0.1, total_reward = 177.0\n",
      "Episode 1510 : epsilon = 0.1, total_reward = 465.0\n",
      "Episode 1520 : epsilon = 0.1, total_reward = 227.0\n",
      "Episode 1530 : epsilon = 0.1, total_reward = 334.0\n",
      "Episode 1540 : epsilon = 0.1, total_reward = 435.0\n",
      "Episode 1550 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1560 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1570 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1580 : epsilon = 0.1, total_reward = 458.0\n",
      "Episode 1590 : epsilon = 0.1, total_reward = 178.0\n",
      "Episode 1600 : epsilon = 0.1, total_reward = 329.0\n",
      "Episode 1610 : epsilon = 0.1, total_reward = 202.0\n",
      "Episode 1620 : epsilon = 0.1, total_reward = 299.0\n",
      "Episode 1630 : epsilon = 0.1, total_reward = 392.0\n",
      "Episode 1640 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1650 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1660 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1670 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1680 : epsilon = 0.1, total_reward = 431.0\n",
      "Episode 1690 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1700 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1710 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1720 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1730 : epsilon = 0.1, total_reward = 447.0\n",
      "Episode 1740 : epsilon = 0.1, total_reward = 338.0\n",
      "Episode 1750 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1760 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1770 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1780 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1790 : epsilon = 0.1, total_reward = 422.0\n",
      "Episode 1800 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1810 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1820 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1830 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1840 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1850 : epsilon = 0.1, total_reward = 460.0\n",
      "Episode 1860 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1870 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1880 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1890 : epsilon = 0.1, total_reward = 413.0\n",
      "Episode 1900 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1910 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1920 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1930 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1940 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1950 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1960 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1970 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 1980 : epsilon = 0.1, total_reward = 366.0\n",
      "Episode 1990 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2000 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2010 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2020 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2030 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2040 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2050 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2060 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2070 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2080 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2090 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2100 : epsilon = 0.1, total_reward = 393.0\n",
      "Episode 2110 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2120 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2130 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2140 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2150 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2160 : epsilon = 0.1, total_reward = 390.0\n",
      "Episode 2170 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2180 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2190 : epsilon = 0.1, total_reward = 398.0\n",
      "Episode 2200 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2210 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2220 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2230 : epsilon = 0.1, total_reward = 477.0\n",
      "Episode 2240 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2250 : epsilon = 0.1, total_reward = 490.0\n",
      "Episode 2260 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2270 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2280 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2290 : epsilon = 0.1, total_reward = 405.0\n",
      "Episode 2300 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2310 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2320 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2330 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2340 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2350 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2360 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2370 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2380 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2390 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2400 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2410 : epsilon = 0.1, total_reward = 398.0\n",
      "Episode 2420 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2430 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2440 : epsilon = 0.1, total_reward = 442.0\n",
      "Episode 2450 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2460 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2470 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2480 : epsilon = 0.1, total_reward = 76.0\n",
      "Episode 2490 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2500 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2510 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2520 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2530 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2540 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2550 : epsilon = 0.1, total_reward = 424.0\n",
      "Episode 2560 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2570 : epsilon = 0.1, total_reward = 435.0\n",
      "Episode 2580 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2590 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2600 : epsilon = 0.1, total_reward = 406.0\n",
      "Episode 2610 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2620 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2630 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2640 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2650 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2660 : epsilon = 0.1, total_reward = 446.0\n",
      "Episode 2670 : epsilon = 0.1, total_reward = 454.0\n",
      "Episode 2680 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2690 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2700 : epsilon = 0.1, total_reward = 323.0\n",
      "Episode 2710 : epsilon = 0.1, total_reward = 361.0\n",
      "Episode 2720 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2730 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2740 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2750 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2760 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2770 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2780 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2790 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2800 : epsilon = 0.1, total_reward = 324.0\n",
      "Episode 2810 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2820 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2830 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2840 : epsilon = 0.1, total_reward = 487.0\n",
      "Episode 2850 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2860 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2870 : epsilon = 0.1, total_reward = 496.0\n",
      "Episode 2880 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2890 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2900 : epsilon = 0.1, total_reward = 359.0\n",
      "Episode 2910 : epsilon = 0.1, total_reward = 192.0\n",
      "Episode 2920 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2930 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2940 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2950 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2960 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2970 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 2980 : epsilon = 0.1, total_reward = 410.0\n",
      "Episode 2990 : epsilon = 0.1, total_reward = 415.0\n",
      "Episode 3000 : epsilon = 0.1, total_reward = 398.0\n",
      "Episode 3010 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3020 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3030 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3040 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3050 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3060 : epsilon = 0.1, total_reward = 333.0\n",
      "Episode 3070 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3080 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3090 : epsilon = 0.1, total_reward = 475.0\n",
      "Episode 3100 : epsilon = 0.1, total_reward = 222.0\n",
      "Episode 3110 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3120 : epsilon = 0.1, total_reward = 402.0\n",
      "Episode 3130 : epsilon = 0.1, total_reward = 108.0\n",
      "Episode 3140 : epsilon = 0.1, total_reward = 366.0\n",
      "Episode 3150 : epsilon = 0.1, total_reward = 381.0\n",
      "Episode 3160 : epsilon = 0.1, total_reward = 395.0\n",
      "Episode 3170 : epsilon = 0.1, total_reward = 448.0\n",
      "Episode 3180 : epsilon = 0.1, total_reward = 350.0\n",
      "Episode 3190 : epsilon = 0.1, total_reward = 447.0\n",
      "Episode 3200 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3210 : epsilon = 0.1, total_reward = 285.0\n",
      "Episode 3220 : epsilon = 0.1, total_reward = 302.0\n",
      "Episode 3230 : epsilon = 0.1, total_reward = 249.0\n",
      "Episode 3240 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3250 : epsilon = 0.1, total_reward = 239.0\n",
      "Episode 3260 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3270 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3280 : epsilon = 0.1, total_reward = 483.0\n",
      "Episode 3290 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3300 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3310 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3320 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3330 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3340 : epsilon = 0.1, total_reward = 383.0\n",
      "Episode 3350 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3360 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3370 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3380 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3390 : epsilon = 0.1, total_reward = 328.0\n",
      "Episode 3400 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3410 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3420 : epsilon = 0.1, total_reward = 394.0\n",
      "Episode 3430 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3440 : epsilon = 0.1, total_reward = 238.0\n",
      "Episode 3450 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3460 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3470 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3480 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3490 : epsilon = 0.1, total_reward = 324.0\n",
      "Episode 3500 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3510 : epsilon = 0.1, total_reward = 370.0\n",
      "Episode 3520 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3530 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3540 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3550 : epsilon = 0.1, total_reward = 476.0\n",
      "Episode 3560 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3570 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3580 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3590 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3600 : epsilon = 0.1, total_reward = 391.0\n",
      "Episode 3610 : epsilon = 0.1, total_reward = 411.0\n",
      "Episode 3620 : epsilon = 0.1, total_reward = 15.0\n",
      "Episode 3630 : epsilon = 0.1, total_reward = 213.0\n",
      "Episode 3640 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3650 : epsilon = 0.1, total_reward = 294.0\n",
      "Episode 3660 : epsilon = 0.1, total_reward = 292.0\n",
      "Episode 3670 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3680 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3690 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3700 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3710 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3720 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3730 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3740 : epsilon = 0.1, total_reward = 488.0\n",
      "Episode 3750 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3760 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3770 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3780 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3790 : epsilon = 0.1, total_reward = 315.0\n",
      "Episode 3800 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3810 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3820 : epsilon = 0.1, total_reward = 407.0\n",
      "Episode 3830 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3840 : epsilon = 0.1, total_reward = 294.0\n",
      "Episode 3850 : epsilon = 0.1, total_reward = 486.0\n",
      "Episode 3860 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3870 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3880 : epsilon = 0.1, total_reward = 286.0\n",
      "Episode 3890 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3900 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3910 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3920 : epsilon = 0.1, total_reward = 274.0\n",
      "Episode 3930 : epsilon = 0.1, total_reward = 328.0\n",
      "Episode 3940 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3950 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3960 : epsilon = 0.1, total_reward = 488.0\n",
      "Episode 3970 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 3980 : epsilon = 0.1, total_reward = 397.0\n",
      "Episode 3990 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4000 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4010 : epsilon = 0.1, total_reward = 352.0\n",
      "Episode 4020 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4030 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4040 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4050 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4060 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4070 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4080 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4090 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4100 : epsilon = 0.1, total_reward = 337.0\n",
      "Episode 4110 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4120 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4130 : epsilon = 0.1, total_reward = 482.0\n",
      "Episode 4140 : epsilon = 0.1, total_reward = 247.0\n",
      "Episode 4150 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4160 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4170 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4180 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4190 : epsilon = 0.1, total_reward = 425.0\n",
      "Episode 4200 : epsilon = 0.1, total_reward = 35.0\n",
      "Episode 4210 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4220 : epsilon = 0.1, total_reward = 424.0\n",
      "Episode 4230 : epsilon = 0.1, total_reward = 306.0\n",
      "Episode 4240 : epsilon = 0.1, total_reward = 415.0\n",
      "Episode 4250 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4260 : epsilon = 0.1, total_reward = 264.0\n",
      "Episode 4270 : epsilon = 0.1, total_reward = 212.0\n",
      "Episode 4280 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4290 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4300 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4310 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4320 : epsilon = 0.1, total_reward = 167.0\n",
      "Episode 4330 : epsilon = 0.1, total_reward = 391.0\n",
      "Episode 4340 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4350 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4360 : epsilon = 0.1, total_reward = 295.0\n",
      "Episode 4370 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4380 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4390 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4400 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4410 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4420 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4430 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4440 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4450 : epsilon = 0.1, total_reward = 470.0\n",
      "Episode 4460 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4470 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4480 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4490 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4500 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4510 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4520 : epsilon = 0.1, total_reward = 405.0\n",
      "Episode 4530 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4540 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4550 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4560 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4570 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4580 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4590 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4600 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4610 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4620 : epsilon = 0.1, total_reward = 318.0\n",
      "Episode 4630 : epsilon = 0.1, total_reward = 452.0\n",
      "Episode 4640 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4650 : epsilon = 0.1, total_reward = 466.0\n",
      "Episode 4660 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4670 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4680 : epsilon = 0.1, total_reward = 285.0\n",
      "Episode 4690 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4700 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4710 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4720 : epsilon = 0.1, total_reward = 166.0\n",
      "Episode 4730 : epsilon = 0.1, total_reward = 282.0\n",
      "Episode 4740 : epsilon = 0.1, total_reward = 124.0\n",
      "Episode 4750 : epsilon = 0.1, total_reward = 444.0\n",
      "Episode 4760 : epsilon = 0.1, total_reward = 10.0\n",
      "Episode 4770 : epsilon = 0.1, total_reward = 19.0\n",
      "Episode 4780 : epsilon = 0.1, total_reward = 101.0\n",
      "Episode 4790 : epsilon = 0.1, total_reward = 16.0\n",
      "Episode 4800 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4810 : epsilon = 0.1, total_reward = 354.0\n",
      "Episode 4820 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4830 : epsilon = 0.1, total_reward = 275.0\n",
      "Episode 4840 : epsilon = 0.1, total_reward = 371.0\n",
      "Episode 4850 : epsilon = 0.1, total_reward = 347.0\n",
      "Episode 4860 : epsilon = 0.1, total_reward = 424.0\n",
      "Episode 4870 : epsilon = 0.1, total_reward = 455.0\n",
      "Episode 4880 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4890 : epsilon = 0.1, total_reward = 418.0\n",
      "Episode 4900 : epsilon = 0.1, total_reward = 425.0\n",
      "Episode 4910 : epsilon = 0.1, total_reward = 324.0\n",
      "Episode 4920 : epsilon = 0.1, total_reward = 302.0\n",
      "Episode 4930 : epsilon = 0.1, total_reward = 394.0\n",
      "Episode 4940 : epsilon = 0.1, total_reward = 351.0\n",
      "Episode 4950 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 4960 : epsilon = 0.1, total_reward = 324.0\n",
      "Episode 4970 : epsilon = 0.1, total_reward = 233.0\n",
      "Episode 4980 : epsilon = 0.1, total_reward = 270.0\n",
      "Episode 4990 : epsilon = 0.1, total_reward = 368.0\n"
     ]
    }
   ],
   "source": [
    "q_values = first_visit_mc_control(environment, gamma=1.0, max_episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant appeler la fonction _display_ définie précédemment pour visualiser la politique _greedy_ définie à partir des valeurs d'actions ($q$-_values_) apprises avec la méthode Monte Carlo :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tabular(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Répétez votre expérience quelques fois. Qu'est-ce que vous observez?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le contrôle par Monte Carlo First-Visit parvient à trouver une politique qui résout l’environnement, c’est-à-dire qui atteint régulièrement la récompense maximale (500). Les différences d’un entraînement à l’autre se situent essentiellement dans la vitesse de convergence de la politique. De manière générale, une fois que la politique a convergé à la performance maximale (500), les performances varient très peu, hormis quelques fluctuations dues à l’exploration. On remarque toutefois que certains épisodes se terminent par un échec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Pourquoi est-ce que les résultats sont différents à chaque répétition? D'où provient la variance d'une répétition à l'autre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le code suivant pour implémenter la stratégie Q-learning. Votre fonction doit recevoir en entrée le taux d'actualisation $\\gamma$ à considérer dans l'objectif, un taux d'apprentissage initial $\\alpha_0$, ainsi que le nombre maximal d'épisodes à effectuer. Votre fonction doit retourner la matrice des valeurs d'actions ($q$-_values_) sous la forme d'un _numpy.array_ de dimension $|\\bar{\\mathcal S}| \\times |\\bar{\\mathcal S}| \\times |\\mathcal A|$, où $\\bar{\\mathcal S}$ correspond à l'ensemble des états discrétisés.\n",
    "\n",
    "Considérez un mécanisme de réduction du taux d'apprentissage au fil des épisodes tel que l'épisode $i$ est effectué avec un taux d'apprentissage $\\alpha = \\alpha_0 \\times 0.99^{i-1}$ tant que $\\alpha \\geq 0.1$.\n",
    "\n",
    "Considérez une politique de collecte de données de type $\\varepsilon$-greedy telle que l'épisode $i$ est réalisé avec un taux d'exploration $\\varepsilon = 0.99^{i-1}$ tant que $\\varepsilon \\geq 0.1$.\n",
    "\n",
    "À tous les 10 épisodes, votre fonction doit afficher le taux d'exploration $\\varepsilon$ utilisé ainsi que la somme des récompenses cumulées (sans actualisation) durant l'épisode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(environment, gamma, learning_rate, max_episodes):\n",
    "    n_buckets = [1, 1, 6, 12]\n",
    "    n = numpy.prod(n_buckets)  # nombre total d'états discrets possibles\n",
    "    k = environment.action_space.n # nombre total d'actions possibles\n",
    "\n",
    "    q_values = numpy.random.rand(n, k) # initialisation aléatoire des q_values\n",
    "\n",
    "    for e in range(max_episodes):\n",
    "        alpha = learning_rate * 0.99 ** e if learning_rate * 0.99 ** e > 0.1 else 0.1 # taux d'apprentissage\n",
    "        epsilon = 0.99 ** e if 0.99 ** e > 0.1 else 0.1 # taux d'exploration\n",
    "\n",
    "        total_reward = 0\n",
    "        fin_episode = False\n",
    "        state, _ = environment.reset()\n",
    "        s_idx = discretize(state, lower_bounds, upper_bounds)\n",
    "        while not fin_episode:\n",
    "            if numpy.random.rand() > epsilon:\n",
    "                action = numpy.argmax(q_values[s_idx]) \n",
    "            else:\n",
    "                action = numpy.random.randint(k)\n",
    "    \n",
    "            next_state, reward, terminated, truncated, _ = environment.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            next_s_idx = discretize(next_state, lower_bounds, upper_bounds)\n",
    "        \n",
    "            # Mise à jour des q_values par Q-learning\n",
    "            q_values[s_idx][action] = q_values[s_idx][action] + alpha * (reward + gamma * numpy.max(q_values[next_s_idx]) - q_values[s_idx][action])\n",
    "\n",
    "            s_idx = next_s_idx\n",
    "            fin_episode = terminated or truncated\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            print(f\"Episode {e} : epsilon = {epsilon}, total_reward = {total_reward}\")\n",
    "    environment.close()\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquez votre stratégie Q-learning au contrôle de l'agent dans CartPole-v1.\n",
    "\n",
    "Générez 500 épisodes sans utiliser d'actualisation (donc avec $\\gamma = 1$) et avec un taux d'apprentissage initial $\\alpha_0 = 1$. Votre agent devrait converger à une récompense cumulative autour de 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 : epsilon = 1.0, total_reward = 18.0\n",
      "Episode 10 : epsilon = 0.9043820750088044, total_reward = 15.0\n",
      "Episode 20 : epsilon = 0.8179069375972308, total_reward = 52.0\n",
      "Episode 30 : epsilon = 0.7397003733882802, total_reward = 13.0\n",
      "Episode 40 : epsilon = 0.6689717585696803, total_reward = 11.0\n",
      "Episode 50 : epsilon = 0.6050060671375364, total_reward = 9.0\n",
      "Episode 60 : epsilon = 0.5471566423907612, total_reward = 31.0\n",
      "Episode 70 : epsilon = 0.49483865960020695, total_reward = 12.0\n",
      "Episode 80 : epsilon = 0.4475232137638106, total_reward = 71.0\n",
      "Episode 90 : epsilon = 0.4047319726783238, total_reward = 60.0\n",
      "Episode 100 : epsilon = 0.3660323412732292, total_reward = 38.0\n",
      "Episode 110 : epsilon = 0.33103308832101386, total_reward = 228.0\n",
      "Episode 120 : epsilon = 0.2993803913123313, total_reward = 120.0\n",
      "Episode 130 : epsilon = 0.27075425951199406, total_reward = 142.0\n",
      "Episode 140 : epsilon = 0.24486529903492948, total_reward = 64.0\n",
      "Episode 150 : epsilon = 0.22145178723886091, total_reward = 293.0\n",
      "Episode 160 : epsilon = 0.2002770268574893, total_reward = 500.0\n",
      "Episode 170 : epsilon = 0.18112695312597024, total_reward = 200.0\n",
      "Episode 180 : epsilon = 0.16380796970808742, total_reward = 500.0\n",
      "Episode 190 : epsilon = 0.14814499154757946, total_reward = 500.0\n",
      "Episode 200 : epsilon = 0.13397967485796172, total_reward = 500.0\n",
      "Episode 210 : epsilon = 0.12116881635704835, total_reward = 500.0\n",
      "Episode 220 : epsilon = 0.10958290556334815, total_reward = 500.0\n",
      "Episode 230 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 240 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 250 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 260 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 270 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 280 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 290 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 300 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 310 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 320 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 330 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 340 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 350 : epsilon = 0.1, total_reward = 364.0\n",
      "Episode 360 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 370 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 380 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 390 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 400 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 410 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 420 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 430 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 440 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 450 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 460 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 470 : epsilon = 0.1, total_reward = 500.0\n",
      "Episode 480 : epsilon = 0.1, total_reward = 437.0\n",
      "Episode 490 : epsilon = 0.1, total_reward = 500.0\n"
     ]
    }
   ],
   "source": [
    "q_values = q_learning(environment, gamma=1.0, learning_rate=1.0, max_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant appeler la fonction _display_ définie précédemment pour visualiser la politique _greedy_ définie à partir des valeurs d'actions ($q$-_values_) apprisent avec la méthode Monte Carlo :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tabular(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparez ces résultats avec ceux obtenus précédemment en utilisant la stratégie First-visit Monte Carlo :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Quel est l'intérêt de réduire le taux d'apprentissage au fil des épisodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
