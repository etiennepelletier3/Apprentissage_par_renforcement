{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environnements simples et solutions exactes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Implémentation d'un environnement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérez l'environnement en grille $5 \\times 5$ suivant :\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\text{A} & \\phantom{\\text{A}} & \\text{B} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\text{B'} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline \\\\[-7pt]\n",
    "\\phantom{\\text{A}} & \\text{A'} & \\phantom{\\text{A}} & \\phantom{\\text{A}} & \\phantom{\\text{A}} \\\\[2pt]\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "L'environnement est caractérisé par l'ensemble d'actions $\\mathcal A := \\{ \\uparrow, \\downarrow, \\rightarrow, \\leftarrow \\}$ visant à déplacer l'agent d'une case dans la direction indiquée par la flèche.\n",
    "\n",
    "Considérez des déplacements déterministes :\n",
    "- si $S_t = \\text{A}$, alors $S_{t+1} = \\text{A'}$ (peu importe l'action);\n",
    "- si $S_t = \\text{B}$, alors $S_{t+1} = \\text{B'}$ (peu importe l'action);\n",
    "- si l'action fait sortir l'agent de la grille, alors $S_{t+1} = S_t$;\n",
    "- sinon appliquer l'action.\n",
    "\n",
    "Considérez la fonction de récompense suivante :\n",
    "- si $S_t = \\text{A}$, alors $R_{t+1} = R_{\\text{A}}$;\n",
    "- si $S_t = \\text{B}$, alors $R_{t+1} = R_{\\text{B}}$;\n",
    "- si l'action fait sortir l'agent de la grille, alors $R_{t+1} = -1$;\n",
    "- sinon $R_{t+1} = 0$,\n",
    "\n",
    "où $R_{\\text{A}}$ et $R_{\\text{B}}$ sont des valeurs spécifiées à l'initialisation de l'environnement.\n",
    "\n",
    "Complétez le code suivant pour implémenter cet environnement. Votre classe doit contenir une fonction _expected\\_reward_ retournant la récompense attendue en suivant la politique spécifiée dans un état donné. Votre classe doit également contenir une fonction _prob\\_transition\\_policy_ retournant la probabilité de transitionner vers un prochain état donné en suivant la politique spécifiée dans un état courant donné.\n",
    "\n",
    "Vous pouvez implémenter d'autres fonctions utilitaires à votre guise. Les manières de représenter les états et la politique sont laissées à votre discrétion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    \n",
    "    def __init__(self, reward_a, reward_b):\n",
    "        self.reward_a = reward_a\n",
    "        self.reward_b = reward_b\n",
    "        \n",
    "        self.rows = 5\n",
    "        self.cols = 5\n",
    "\n",
    "        self.A = (0, 1)      # position de A\n",
    "        self.A_prime = (4, 1)  # position de A'\n",
    "        self.B = (0, 3)      # position de B\n",
    "        self.B_prime = (2, 3)  # position de B'\n",
    "\n",
    "        self.actions = {\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0),\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1)\n",
    "        }\n",
    "\n",
    "    def step(self, state, action):\n",
    "\n",
    "        # Si state est A ou B\n",
    "        if state == self.A:\n",
    "            return self.A_prime, self.reward_a\n",
    "        if state == self.B:\n",
    "            return self.B_prime, self.reward_b\n",
    "        \n",
    "        # Mouvement standard : calcul de la nouvelle position\n",
    "        delta = self.actions[action]\n",
    "        next_state = (state[0] + delta[0], state[1] + delta[1])\n",
    "        \n",
    "        # Vérifier si le nouvel état est hors grille\n",
    "        if not (0 <= next_state[0] < self.rows and 0 <= next_state[1] < self.cols):\n",
    "            # Si l'agent sort de la grille, il reste au même endroit et reçoit -1\n",
    "            return state, -1\n",
    "        \n",
    "        # Sinon, le déplacement est normal et la récompense est nulle\n",
    "        return next_state, 0\n",
    "    \n",
    "    def expected_reward(self, state, policy):\n",
    "        # Pour les états A et B, la récompense est déterminée indépendamment de l'action\n",
    "        if state == self.A:\n",
    "            return self.reward_a\n",
    "        if state == self.B:\n",
    "            return self.reward_b\n",
    "    \n",
    "        exp_reward = 0\n",
    "        # Parcourir toutes les actions possibles dans l'état selon la politique\n",
    "        for action, prob in policy[state].items():\n",
    "            _, reward = self.step(state, action)\n",
    "            exp_reward += prob * reward\n",
    "        return exp_reward\n",
    "    \n",
    "    def prob_transition_policy(self, next_state, state, policy):\n",
    "        # Pour les états spéciaux A et B, le prochain état est fixé\n",
    "        if state == self.A:\n",
    "            return 1 if next_state == self.A_prime else 0\n",
    "        if state == self.B:\n",
    "            return 1 if next_state == self.B_prime else 0\n",
    "        \n",
    "        total_prob = 0\n",
    "        # Somme des probabilités de toutes les actions conduisant à next_state\n",
    "        for action, prob in policy[state].items():\n",
    "            ns, _ = self.step(state, action)\n",
    "            if ns == next_state:\n",
    "                total_prob += prob\n",
    "        return total_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expliquez comment vous représentez une politique et comment vous représentez les états dans votre implémentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10] points] Résolution exacte du système linéaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résolvez le système linéaire résultant de cet environnement pour calculer la fonction de valeur d'état associée à la politique $\\pi(a | s) = 1/4$ pour tout $a \\in \\mathcal A$, $s \\in \\mathcal S$. Considérez un taux d'actualisation $\\gamma = 0.9$.\n",
    "\n",
    "Affichez votre résultat sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.309  8.789  4.428  5.322  1.492]\n",
      " [ 1.522  2.992  2.25   1.908  0.547]\n",
      " [ 0.051  0.738  0.673  0.358 -0.403]\n",
      " [-0.974 -0.435 -0.355 -0.586 -1.183]\n",
      " [-1.858 -1.345 -1.229 -1.423 -1.975]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "reward_A = 10\n",
    "reward_B = 5\n",
    "\n",
    "grid = Gridworld(reward_A, reward_B)\n",
    "\n",
    "states = [(i, j) for i in range(grid.rows) for j in range(grid.cols)]\n",
    "n_states = len(states)\n",
    "\n",
    "def state_to_index(state):\n",
    "    \"\"\"Associe à un état (i,j) son indice dans [0, n_states - 1].\"\"\"\n",
    "    i, j = state\n",
    "    return i * grid.cols + j\n",
    "\n",
    "# --- Construction du système linéaire A * V = b ---\n",
    "A_mat = numpy.zeros((n_states, n_states))\n",
    "b_vec = numpy.zeros(n_states)\n",
    "\n",
    "for state in states:\n",
    "    idx = state_to_index(state)\n",
    "\n",
    "    # Cas particuliers des états A et B (transitions déterministes)\n",
    "    if state == grid.A:\n",
    "        A_mat[idx, idx] = 1\n",
    "        A_mat[idx, state_to_index(grid.A_prime)] = -gamma\n",
    "        b_vec[idx] = reward_A\n",
    "    elif state == grid.B:\n",
    "        A_mat[idx, idx] = 1\n",
    "        A_mat[idx, state_to_index(grid.B_prime)] = -gamma\n",
    "        b_vec[idx] = reward_B\n",
    "\n",
    "    else:\n",
    "        # Pour un état ordinaire, on part de l'équation : \n",
    "        # V(s) = (1/4) * somme_{a}[ r(s,a) + gamma * V(s'(s,a)) ]\n",
    "        # On écrit cela sous forme: V(s) - (gamma/4) * somme_{a} V(s'(s,a)) = (1/4) somme_{a} r(s,a)\n",
    "        A_mat[idx, idx] = 1  # coefficient de V(s)\n",
    "        for action in grid.actions.keys():\n",
    "            next_state, r = grid.step(state, action)\n",
    "            idx_ns = state_to_index(next_state)\n",
    "            A_mat[idx, idx_ns] -= gamma/4\n",
    "            b_vec[idx] += r/4\n",
    "\n",
    "V = numpy.linalg.solve(A_mat, b_vec)\n",
    "V_matrix = V.reshape(grid.rows, grid.cols)\n",
    "print(numpy.round(V_matrix, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmation dynamique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Évaluation de politique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le code suivant pour implémenter la stratégie d'évaluation de politique par programmation dynamique. Votre fonction doit recevoir en entrée l'environnement à résoudre, la politique à évaluer, le taux d'actualisation $\\gamma$, ainsi qu'un seuil de tolérance à l'erreur $\\theta$ en-dessous duquel l'algorithme peut terminer. Votre fonction doit retourner la fonction de valeur d'état sous la forme d'un vecteur de longueur $|\\mathcal S|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(gridworld, pi, gamma, theta):\n",
    "    rows, cols = gridworld.rows, gridworld.cols\n",
    "    n_states = rows * cols\n",
    "    V = numpy.zeros(n_states)\n",
    "\n",
    "    delta = float('inf')\n",
    "    while delta >= theta:\n",
    "        delta = 0\n",
    "        # Parcourir tous les états\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                state = (i, j)\n",
    "                idx = state_to_index(state)\n",
    "                v = V[idx]\n",
    "                new_v = 0\n",
    "                # Calcul de l'espérance sur toutes les actions selon la politique pi\n",
    "                for action, prob in pi[state].items():\n",
    "                    next_state, reward = gridworld.step(state, action)\n",
    "                    new_v += prob * (reward + gamma * V[state_to_index(next_state)])\n",
    "                V[idx] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "    return V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez votre méthode pour évaluer la valeur des états sous la politique $\\pi(a|s) = 1/4$ pour tout $a \\in \\mathcal A$, $s \\in \\mathcal S$, dans l'environnement en grille implémenté précédemment. Considérez un taux d'actualisation $\\gamma = 0.9$ et un seuil de tolérance $\\theta = 10^{-5}$.\n",
    "\n",
    "Affichez votre résultat sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.309  8.789  4.428  5.322  1.492]\n",
      " [ 1.522  2.992  2.25   1.908  0.547]\n",
      " [ 0.051  0.738  0.673  0.358 -0.403]\n",
      " [-0.974 -0.435 -0.355 -0.586 -1.183]\n",
      " [-1.858 -1.345 -1.229 -1.423 -1.975]]\n"
     ]
    }
   ],
   "source": [
    "reward_A = 10\n",
    "reward_B = 5\n",
    "gamma = 0.9\n",
    "theta = 1e-5\n",
    "\n",
    "grid = Gridworld(reward_A, reward_B)\n",
    "\n",
    "policy = {}\n",
    "for i in range(grid.rows):\n",
    "    for j in range(grid.cols):\n",
    "        # Pour chaque état (i,j), la probabilité de prendre n'importe quelle action est 1/4\n",
    "        policy[(i, j)] = {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
    "\n",
    "V = policy_evaluation(grid, policy, gamma, theta)\n",
    "\n",
    "# Conversion du vecteur V en une matrice 5x5 (ordre row-major)\n",
    "V_matrix = V.reshape((grid.rows, grid.cols))\n",
    "print(numpy.round(V_matrix, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Itération de valeur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le code suivant pour implémenter la stratégie d'itération de valeurs permettant d'identifier la fonction de valeur de la politique optimale par programmation dynamique. Votre fonction doit recevoir en entrée l'environnement à résoudre, le taux d'actualisation $\\gamma$, ainsi qu'un seuil de tolérance à l'erreur $\\theta$ en-dessous duquel l'algorithme peut terminer. Votre fonction doit retourner la fonction de valeur d'état sous la forme d'un vecteur de longueur $|\\mathcal S|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gridworld, gamma, theta):\n",
    "    pass # à compléter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez votre méthode pour évaluer la valeur des états sous la politique optimale dans l'environnement en grille implémenté précédemment. Considérez un taux d'actualisation $\\gamma = 0.9$ et un seuil de tolérance $\\theta = 10^{-5}$.\n",
    "\n",
    "Affichez votre résultat sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Énumérez toutes les situations dans lesquelles un agent suivant la politique optimale traverse l'état $\\text{B}$. Justifiez votre réponse en référant aux valeurs d'états affichées précédemment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Évaluez maintenant la valeur des états sous la politique optimale en considérant un taux d'actualisation $\\gamma = 0.2$ (conserver le même seuil de tolérance $\\theta = 10^{-5}$).\n",
    "\n",
    "Affichez votre résultat sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Expliquez l'impact de cette réduction taux d'actualisation sur la politique optimale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Quelle est la valeur entière minimale de $R_{\\text{A}}$ permettant de retrouver, avec $\\gamma = 0.2$, la même politique optimale que celle calculée précédemment avec $\\gamma = 0.9$?\n",
    "\n",
    "Supportez votre réponse en affichant les valeurs d'états sous la forme d'une matrice $5 \\times 5$, où la cellule $(i, j)$ correspond à l'emplacement $(i, j)$ dans l'environnement en grille illustré plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ L'expérience précédente permet d'exposer une relation entre l'amplitude des récompenses, la valeur du taux d'actualisation et la politique optimale résultante. Décrivez cette relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrôle tabulaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Environnement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant permet d'instancier un environnement de type CartPole-v1 provenant de la librairie Gymnasium d'OpenAI :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "environment = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet environnement, le but de l'agent consiste à déplacer une plateforme de manière à faire tenir en équilibre un poteau sur la plateforme. Les états sont des vecteurs représentant la position de la plateforme et du poteau. La récompense est de 1 pour chaque état où le poteau est encore sur la plateforme et tombe à 0 lorsque le poteau tombe, ce qui sonne la fin de l'épisode.\n",
    "\n",
    "L'environnement dispose de différents attributs permettant, par exemple, de retrouver la dimension d'un état $s \\in \\mathcal S$ et la dimension de l'espace d'actions $\\mathcal A$ dans l'environnement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'environnement CartPole-v1 a des observations de la forme (4,) et contient 2 actions.\n"
     ]
    }
   ],
   "source": [
    "# observation_space : environment attribute qualifying the state space\n",
    "# continuous state space : shape attribute\n",
    "observation_dim = environment.observation_space.shape\n",
    "\n",
    "# action_space : environment attribute qualifying the action space\n",
    "# discrete action space : n attribute indicating the number of actions\n",
    "# actions : 0...(n-1)\n",
    "n_actions = environment.action_space.n # only discrete action spaces\n",
    "\n",
    "print(f\"L'environnement {environment.spec.id} a des états de la forme {observation_dim} et contient {n_actions} actions.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À quoi correspondent les éléments d'un vecteur d'état dans cet environnement?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les besoins de l'implémentation tabulaire, il est nécessaire de discrétiser l'espace d'états continu de CartPole. Le code suivant permet de déterminer les limites de l'espace d'état :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians\n",
    "\n",
    "# get upper bounds on state components\n",
    "upper_bounds = environment.observation_space.high\n",
    "\n",
    "# recall : [cart position, cart speed, pole angle, pole speed]\n",
    "upper_bounds[1] = 0.5\n",
    "upper_bounds[3] = radians(50)\n",
    "\n",
    "# symmetric state space\n",
    "lower_bounds = -upper_bounds # or environment.observation_space.low with similar trick as above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme les vitesses ont des bornes infinies qui ne se prètent pas bien à la discrétisation, on leur fixe les valeurs limites de 0.5 unité / $t$ et 50 radians / $t$ pour la vitesse du chariot et du poteau, respectivement.\n",
    "\n",
    "Complétez le code suivant pour implémenter une fonction permettant de discrétiser un état. Votre fonction doit recevoir en entrée l'état à discrétiser, les bornnes inférieures et supérieures à considérer pour chaque composante d'un état, ainsi  qu'une liste indiquant le nombre de zones (_buckets_ ou _bins_) à utiliser pour discrétiser chaque composante. Chaque zone est de taille identique : $(u_i - \\ell_i) / N_i$, où $u_i$ et $\\ell_i$ sont les bornes supérieure et inférieure pour la composante $i$ et $N_i$ est le nombre de zones qui divisent la composante $i$. Votre fonction doit retourner un tuple de taille 4, où chaque indice correspond à l'indice de la zone discrète représentant la composante continue Assurez-vous de gérer les cas limites où l'état en entrée contient des composantes en dehors de vos bornes (ramenez-les à la borne la plus près). Considérez la discrétisation par défaut en 1, 1, 6 et 12 zones par composante respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(state, lower_bounds, upper_bounds, n_buckets=[1, 1, 6, 12]):\n",
    "    '''\n",
    "    state : (4,) numpy array\n",
    "    lower_bounds : (4,) numpy array\n",
    "    upper_bounds : (4,) numpy array\n",
    "    n_buckets : list of integers\n",
    "\n",
    "    Return : Discrete state index (integer).\n",
    "    '''\n",
    "    pass # à compléter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ La discrétisation suggérée est seulement appliquée sur la position et la vitesse du poteau. Pourquoi est-ce que cette discrétisation est suffisante pour l'apprentissage d'une bonne politique?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet de visualiser une politique _greedy_ définie sur des valeurs d'actions ($q$-_values_) données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tabular(q_values, max_steps=1000):\n",
    "    '''\n",
    "    q_values : (n, k) numpy array where n is the number of (discrete) states and k is the number of actions\n",
    "    '''\n",
    "    environment = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "    time_step = 0\n",
    "    terminated, truncated = False, False\n",
    "    state, _ = environment.reset()\n",
    "    s_idx = discretize(state, lower_bounds, upper_bounds)\n",
    "    while not (terminated or truncated) and time_step < max_steps:\n",
    "        environment.render()\n",
    "        action = numpy.argmax(q_values[s_idx])\n",
    "        next_state, _, terminated, truncated, _ = environment.step(action)            \n",
    "        next_s_idx = discretize(next_state, lower_bounds, upper_bounds)\n",
    "        s_idx = next_s_idx\n",
    "        time_step += 1\n",
    "    environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elle sera utilisée dans les sections suivantes pour visualiser les politiques apprises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Apprentissage Monte Carlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le code suivant pour implémenter la stratégie First-visit Monte Carlo pour le contrôle. Votre fonction doit recevoir en entrée l'environnement à aborder, le taux d'actualisation $\\gamma$ à considérer dans l'objectif, ainsi que le nombre maximal d'épisodes à effectuer. Votre fonction doit retourner la matrice des valeurs d'actions ($q$-_values_) sous la forme d'un _numpy.array_ de dimension $|\\mathcal S| \\times |\\mathcal A|$, où $\\mathcal S$ correspond à l'ensemble des états discrétisés.\n",
    "\n",
    "Considérez une politique de collecte de données de type $\\varepsilon$-greedy. Pour chaque épisode $i = 1...N$, considérez un taux d'exploration $\\varepsilon = 0.99^{i-1}$ tant que $\\varepsilon \\geq 0.1$.\n",
    "\n",
    "À tous les 10 épisodes, votre fonction doit afficher le taux d'exploration $\\varepsilon$ utilisé ainsi que la somme des récompenses cumulées (sans actualisation) durant l'épisode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_control(environment, gamma, max_episodes):\n",
    "    pass # à compléter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquez votre stratégie First-visit Monte Carlo au contrôle de l'agent dans CartPole-v1.\n",
    "\n",
    "Générez 5000 épisodes sans utiliser d'actualisation (donc avec $\\gamma = 1$). Votre agent devrait converger à une récompense cumulative autour de 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant appeler la fonction _display_ définie précédemment pour visualiser la politique _greedy_ définie à partir des valeurs d'actions ($q$-_values_) apprises avec la méthode Monte Carlo :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Répétez votre expérience quelques fois. Qu'est-ce que vous observez?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Pourquoi est-ce que les résultats sont différents à chaque répétition? D'où provient la variance d'une répétition à l'autre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le code suivant pour implémenter la stratégie Q-learning. Votre fonction doit recevoir en entrée le taux d'actualisation $\\gamma$ à considérer dans l'objectif, un taux d'apprentissage initial $\\alpha_0$, ainsi que le nombre maximal d'épisodes à effectuer. Votre fonction doit retourner la matrice des valeurs d'actions ($q$-_values_) sous la forme d'un _numpy.array_ de dimension $|\\bar{\\mathcal S}| \\times |\\bar{\\mathcal S}| \\times |\\mathcal A|$, où $\\bar{\\mathcal S}$ correspond à l'ensemble des états discrétisés.\n",
    "\n",
    "Considérez un mécanisme de réduction du taux d'apprentissage au fil des épisodes tel que l'épisode $i$ est effectué avec un taux d'apprentissage $\\alpha = \\alpha_0 \\times 0.99^{i-1}$ tant que $\\alpha \\geq 0.1$.\n",
    "\n",
    "Considérez une politique de collecte de données de type $\\varepsilon$-greedy telle que l'épisode $i$ est réalisé avec un taux d'exploration $\\varepsilon = 0.99^{i-1}$ tant que $\\varepsilon \\geq 0.1$.\n",
    "\n",
    "À tous les 10 épisodes, votre fonction doit afficher le taux d'exploration $\\varepsilon$ utilisé ainsi que la somme des récompenses cumulées (sans actualisation) durant l'épisode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(environment, gamma, learning_rate, max_episodes):\n",
    "    pass # à compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquez votre stratégie Q-learning au contrôle de l'agent dans CartPole-v1.\n",
    "\n",
    "Générez 500 épisodes sans utiliser d'actualisation (donc avec $\\gamma = 1$) et avec un taux d'apprentissage initial $\\alpha_0 = 1$. Votre agent devrait converger à une récompense cumulative autour de 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant appeler la fonction _display_ définie précédemment pour visualiser la politique _greedy_ définie à partir des valeurs d'actions ($q$-_values_) apprisent avec la méthode Monte Carlo :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparez ces résultats avec ceux obtenus précédemment en utilisant la stratégie First-visit Monte Carlo :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[IFT-7201]__ Quel est l'intérêt de réduire le taux d'apprentissage au fil des épisodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
